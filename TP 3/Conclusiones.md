# Setup
This project uses Python version `3.8`.

From the project root directory, on a [virtual Python environment](https://virtualenvwrapper.readthedocs.io/en/latest/) (or not, if you're feeling brave), run:
```bash
pip3 install -r requirements.txt
```

Make sure that the source directory is added to your `$PYTHONPATH` environment variable.

# Ejercicio 1
Entrego dos programas: `nb_n.py`, el programa original y `nb_n_gaussianas.py`, el programa modificado para outputear las medias y varianzas de las gaussianas.

Los resultados de las gaussianas para 10000 casos de `paralelo` con `d = 2` y `C = 0.78` fueron:
* Clase 0. Atributo 0. Media: -0.9896316451999999. Varianza: 0.6103653354603021
* Clase 0. Atributo 1. Media: 0.0040217697999999965. Varianza: 0.6174734188434824
* Clase 1. Atributo 0. Media: 0.9977024599999998. Varianza: 0.6038513231113908
* Clase 1. Atributo 1. Media: -0.0020161586000000016. Varianza: 0.5860021197584094

La varianza deber칤a dar `0.6084`. Los valores son muy cercanos as칤 que me creo que funciona bien.


# Ejercicio 2

## ANN
![ANNAvgGraph](2/ANNAvgGraph.png)
![ANNMedianGraph](2/ANNMedianGraph.png)

## Decision Tree
![treeGraph](2/treeGraph.png)

## Naive Bayes
![NBAvgGraph](2/NBAvgGraph.png)
![NBMedianGraph](2/NBMedianGraph.png)

## Conclusiones
Observo que al igual que en los dem치s modelos, al aumentar el n칰mero de dimensiones es inevitable el sobreajuste. Hay "tan pocos puntos" (ya que se vuelve mucho menos denso) que todos los modelos aprenden clasificaciones incorrectas para ciertas regiones donde en el conjunto de entrenamiento hay una mayor칤a "infortuita" (casual y poco representativa de la distribuci칩n) de puntos de una clase. Esto se ve reflejado en el decrecimiento del error de training y el crecimiento de error de test.

Es muy interesante la curva del error de entrenamiento diagonal. Por la forma de aprender de Naive Bayes, supuse que ambas distribuciones le resultar칤an igualmente dif칤ciles (como vemos en las curvas de test). Pero en el diagonal el error es mayor en 4 y 8 dimensiones (antes de que el sobreajuste se haga notorio). No entiendo por qu칠 es mejor que el paralelo cuando en diagonal hay correlaci칩n entre las coordenadas (y nosotros suponemos que no), y en paralelo no la hay.

En general, veo que es un buen problema para resolver con Naive Bayes (asumo porque cumple la condici칩n de "me parezco a lo que tengo cerca").


# Ejercicio 3

## `dos_elipses`
Mediana de 10 runs (seg칰n error de test):

```
Errores:
Entrenamiento:22.0%
Validacion:24.555555555555557%
Test:23.35%
```

![dos_elipsesGraph](3/dos_elipsesGraph.png)

Entre ambas elipses hay puntos de la otra clase. Al tener una gaussiana para cada dimensi칩n, ambas elipses dan la impresi칩n de que la probabilidad m치xima de la clase est치 en el centro del rect치ngulo. Esto resulta en un gran error (en comparaci칩n a otros m칠todos) y una predicci칩n con una sola mancha en el centro.


## `espirales_anidadas`
Mediana de 10 runs (seg칰n error de test):

```
Errores:
Entrenamiento:44.0%
Validacion:42.11111111111111%
Test:44.26%
```

![espirales_anidadasGraph](3/espirales_anidadasGraph.png)

El resultado es nefasto, porque para cada dimensi칩n las clases tienen la misma Gaussiana, resulta muy similar a tirar una moneda. Supongo que la divisi칩n que escogi칩 es producto de la distribuci칩n particular de los datos de entrenamiento.

# Ejercicio 4
Entrego dos programas: `nb_n_histogram.py`, el programa original y `nb_n_histogram_optimize.py`. El primero usa el n칰mero de bins del input. El segundo, adem치s intenta encontrar un n칰mero de bins 칩ptimo.

Para ambos datasets se corrieron 10 ejecuciones para cada cantidad de bins en el intervalo `[1, 200]` y se grafic칩 la mediana de los errores (para cada cantidad de bins).

![SmallNBdos_elipsesMedianGraph](4/SmallNBdos_elipsesMedianGraph.png)

![SmallNBespirales_anidadasMedianGraph](4/SmallNBespirales_anidadasMedianGraph.png)

Aumentar la cantidad de bins (cosa que los achica) con la misma cantidad de puntos, aumenta la probabilidad de que casi todos los puntos en un bin sean de una sola clase. Es decir, a m치s bins hay m치s probabilidad de sobreajuste. Esto se ve claramente en los gr치ficos con rango `[20, 200]`:

![NBdos_elipsesMedianGraph](4/NBdos_elipsesMedianGraph.png)

![NBespirales_anidadasMedianGraph](4/NBespirales_anidadasMedianGraph.png)


## `dos_elipses`
![dos_elipsesGraph](4/dos_elipsesGraph.png)

La forma del espacio de soluciones es ahora mucho m치s favorable. Los puntos de las elipses est치n claramente en s칩lo algunos bins, tanto para los horizontales como los verticales. Es por esto que si bien el modelo no predice elipses, logr칩 aprender dos manchas separadas de puntos (por lo que ahora predice con mucha mayor correctitud los puntos entre las elipses).

## `espirales_anidadas`
![espirales_anidadasGraph](4/espirales_anidadasGraph.png)

Por c칩mo es la distribuci칩n de las clases, hay muchos bins para los cuales hay cerca de la mitad de puntos de cada clase. Esto ocurre porque un bin es un rect치ngulo horizontal o vertical que recorre todo el espacio (asumimos independientes los atributos).

Es por esto que si bien la predicci칩n mejora, el error sigue siendo alto en comparaci칩n a otros m칠todos.

# Ejercicio 5
Entrego dos programas: `b_n_histogram.py`, el programa original y `b_n_histogram_optimize.py`. El primero usa el n칰mero de bins del input. El segundo, adem치s intenta encontrar un n칰mero de bins 칩ptimo.

![SmallBespirales_anidadasMedianGraph](5/SmallBespirales_anidadasMedianGraph.png)

![espirales_anidadasGraph](5/espirales_anidadasGraph.png)

Se ve una clara mejor칤a a simple vista en el gr치fico del archivo `.predic` (y en el gr치fico de errores en general). Esto se debe a que `espirales_anidadas` es un dataset donde la clase depende de la relaci칩n entre ambas dimensiones.

Para n칰mero de bins muy grande se observa el mismo fen칩meno que en el ejercicio anterior, s칩lo que m치s acentuado.

![Bespirales_anidadasMedianGraph](5/Bespirales_anidadasMedianGraph.png)

Esta implementaci칩n anda bien pero tiene el problema de que el c치lculo de la probabilidad de que un punto pertenezca a una clase se vuelve cuadr치tico en la cantidad de features. Esto se ve en el m칠todo `ClassHistogram.probability`, que hace un c치lculo lineal en la cantidad de features y luego una llamada recursiva con un feature menos:

```python
def probability(self, attributeList: List[float]) -> float:
    if attributeList:
        binList = [self.calculate_bin(attribute, value) for attribute, value in enumerate(attributeList)]
        # P(x1, ..., xn | c) = P(x1 | x2, ..., xn, c) * P(x2, ..., xn | c)
        return self.conditional_probability(binList) * self.probability(
            attributeList[1:]
        )
    else:
        return 1
```

Si no me equivoco, la complejidad final ser칤a del orden de `O(n * d**2 * c)` siendo `n` la cantidad de puntos, `d` la cantidad de dimensiones y `c` la cantidad de clases. Para datasets no peque침os (sobre todo en dimensi칩n) es inaplicable.

# Ejercicio 6

Bayes volvi칩 a ser Naive para este ejercicio. No tiene sentido usar un conjunto de validaci칩n en este caso, ya que la cantidad de bins est치 dada por el algoritmo (y no es un par치metro a optimizar). Consecuentemente, entrego un solo programa nuevo, `nb_n_entropy_histogram.py` basado en `nb_n_histogram.py` del ejercicio 4.

Tuve que cambiar bastante la estructura del programa (游땩), porque estaba usando un objeto `Histogram` para cada clase de los datos. Reescrib칤 una parte grande con `pandas` porque pas칩 casi un a침o y aprend칤 cosas nuevas. No fui muy feliz haciendo esa mezcla de c칩digo viejo y nuevo, aunque reconozco que me da una sensaci칩n de progreso el pensar que hace un a침o no entend칤a nada (seguro ahora tampoco entiendo tanto, pero s칤 entiendo m치s).

```
Errores:
Entrenamiento: 3.5000000000000004%
Test: 3.95%
```

Pero vali칩 la pena! Dio incre칤blemente bien. Me resulta muy interesante que elegir bins "inteligentemente" haya mejorado tanto los errores. Chusmeando los bins vi que se concentran en los intervalos de la coordenada donde est치n las elipses. Como cierre dejo la predicci칩n graficada:

![dos_elipses](6/dos_elipses.png)

Los bins andan hermoso. S칤 se nota el problema de la dependencia de las coordenadas, al asumirlas independientes grafica dos rect치ngulos en los intervalos de coordenadas donde estaban las elipses.